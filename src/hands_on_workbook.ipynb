{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2c6033a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRUD and preprocess\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#Vectorize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import spacy\n",
    "\n",
    "#general\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quick Installation : \")\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install contractions\n",
    "# !pip install nltk\n",
    "# !pip install sklearn\n",
    "# !pip install gensim\n",
    "\n",
    "os.system(\"python -m spacy download en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ee3049",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5a1114ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are you guys doing today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcome to the Hands-on Session ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MS Teams is fun, isn't ? team building LOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I would rather come in person ! in person is m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love swimming and playing badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I miss Indian food and swim pool, but food the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0                      How are you guys doing today?\n",
       "1                  Welcome to the Hands-on Session ?\n",
       "2         MS Teams is fun, isn't ? team building LOL\n",
       "3  I would rather come in person ! in person is m...\n",
       "4              I love swimming and playing badminton\n",
       "5  I miss Indian food and swim pool, but food the..."
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = {\n",
    "    'Text' : [\n",
    "        \"How are you guys doing today?\",\n",
    "        \"Welcome to the Hands-on Session ?\",\n",
    "        \"MS Teams is fun, isn't ? team building LOL\",\n",
    "        \"I would rather come in person ! in person is much better\",\n",
    "        \"I love swimming and playing badminton\",\n",
    "        \"I miss Indian food and swim pool, but food the most\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "data_df = pd.DataFrame(data_dict)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "20e10e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textPreprocessing(text):\n",
    "\n",
    "    \"\"\"\n",
    "    input : text (str)\n",
    "    function : perform text pre-processing :\n",
    "              1. Case Generalization\n",
    "              2. Remove Contractions\n",
    "              3. Punctuation Removal\n",
    "              4. Tokenization\n",
    "              5. Stemming\n",
    "              6. Stopword Removal\n",
    "    output : text (str)\n",
    "    \"\"\"\n",
    "\n",
    "    #converting to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    #removing contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #Punctuation Removal\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    #tokenizaton\n",
    "    token_list = nltk.WordPunctTokenizer().tokenize(text)  \n",
    "\n",
    "    #stemming\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    token_list = [ps.stem(token) for token in token_list]\n",
    "    \n",
    "    #removing stopwords\n",
    "    stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "    sanitized_token_list = [word for word in token_list if word not in stopwords_list]\n",
    "    \n",
    "    return \" \".join(sanitized_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b39b9c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Processed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are you guys doing today?</td>\n",
       "      <td>guy today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcome to the Hands-on Session ?</td>\n",
       "      <td>welcom handson session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MS Teams is fun, isn't ? team building LOL</td>\n",
       "      <td>ms team fun team build lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I would rather come in person ! in person is m...</td>\n",
       "      <td>would rather come person person much better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love swimming and playing badminton</td>\n",
       "      <td>love swim play badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I miss Indian food and swim pool, but food the...</td>\n",
       "      <td>miss indian food swim pool food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                      How are you guys doing today?   \n",
       "1                  Welcome to the Hands-on Session ?   \n",
       "2         MS Teams is fun, isn't ? team building LOL   \n",
       "3  I would rather come in person ! in person is m...   \n",
       "4              I love swimming and playing badminton   \n",
       "5  I miss Indian food and swim pool, but food the...   \n",
       "\n",
       "                                Processed_Text  \n",
       "0                                    guy today  \n",
       "1                       welcom handson session  \n",
       "2                   ms team fun team build lol  \n",
       "3  would rather come person person much better  \n",
       "4                     love swim play badminton  \n",
       "5              miss indian food swim pool food  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Processed_Text'] = data_df.Text.apply(textPreprocessing)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee955c",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "64933ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "# X = vectorizer.fit_transform(data_df['Text'])\n",
    "\n",
    "# count_matrix = X.toarray()\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# count_df = pd.DataFrame(count_matrix, columns=feature_names)\n",
    "\n",
    "# print(\"Binary Vectorizer Count Matrix:\")\n",
    "# print(count_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "811bef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(binary = True)\n",
    "\n",
    "# X = vectorizer.fit_transform(data_df['Text'])\n",
    "\n",
    "# count_matrix = X.toarray()\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# count_df = pd.DataFrame(count_matrix, columns=feature_names)\n",
    "\n",
    "# print(\"Binary Vectorizer Count Matrix:\")\n",
    "# print(count_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229112f3",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f81072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeIdfDict(docs, set_of_words):\n",
    "    \n",
    "    idf_dict = dict()\n",
    "    corpus_size = len(docs)\n",
    "\n",
    "    for word in set_of_words:\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        for doc in docs:\n",
    "            \n",
    "            if(word in doc.split(\" \")):\n",
    "                count += 1\n",
    "        \n",
    "        idf_dict[word] = corpus_size/count\n",
    "       \n",
    "        \n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cdc22a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfConversion(docs):\n",
    "    \n",
    "    \"\"\"\n",
    "    input : list of documents (pandas.Series /List)\n",
    "    function : return tf-idf vectors\n",
    "    output : numpy.ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus_size = len(docs)\n",
    "    set_of_words = set()\n",
    "    \n",
    "    for idx in range(corpus_size):\n",
    "        \n",
    "        docs[idx] = textPreprocessing(docs[idx])\n",
    "        set_of_words = set_of_words.union(set(docs[idx].split(\" \")))\n",
    "    \n",
    "    vocab_size = len(set_of_words)\n",
    "    tfidf_mat = np.zeros(shape=(len(docs), vocab_size))\n",
    "    \n",
    "    idf_dict = makeIdfDict(docs, set_of_words)\n",
    "    \n",
    "    list_of_words = list(set_of_words)\n",
    "    \n",
    "    for index in range(corpus_size):\n",
    "    \n",
    "        doc_list = docs[index].split(\" \")\n",
    "        \n",
    "        for idx in range(len(list_of_words)):\n",
    "            \n",
    "            tf = doc_list.count(list_of_words[idx])/len(list_of_words[idx])\n",
    "            idf = np.log2(idf_dict[list_of_words[idx]])\n",
    "            tfidf_mat[index][idx] = tf * idf\n",
    "        \n",
    "    \n",
    "    return tfidf_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "422871f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidfConversion(data_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "352d9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tfidf_matrix = vectorizer.fit_transform(data_df['Text'])\n",
    "# tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "# tfidf_df = pd.DataFrame(tfidf_array, columns=feature_names)\n",
    "\n",
    "# print(\"TF-IDF Matrix:\")\n",
    "# print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a5a7a",
   "metadata": {},
   "source": [
    "### Word to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ce732787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 0.11321540921926498), ('live', 0.045143432915210724), ('rules', 0.04355676844716072), ('kingdom', -0.014120015315711498), ('the', -0.06369484961032867)]\n"
     ]
    }
   ],
   "source": [
    "# import gensim.downloader as api\n",
    "\n",
    "# model = api.load('word2vec-google-news-300')\n",
    "# similar_words = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "# print(similar_words)\n",
    "\n",
    "\n",
    "example_corpus = [\n",
    "    \"king is a man\",\n",
    "    \"queen is a woman\",\n",
    "    \"king rules the kingdom\",\n",
    "    \"queen rules the kingdom\",\n",
    "    \"king and queen live happily\"\n",
    "]\n",
    "\n",
    "model = Word2Vec([sentence.split() for sentence in example_corpus], vector_size=50, window=10, min_count=1, sg=1)\n",
    "\n",
    "similar_words = model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)\n",
    "\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "024c4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(text, model):\n",
    "    \n",
    "    tokens = nltk.tokenize.word_tokenize(text.lower())\n",
    "    text_vector = np.zeros(model.vector_size)\n",
    "    word_count = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            text_vector += model.wv[word]\n",
    "            word_count += 1\n",
    "    if word_count > 0:\n",
    "        text_vector /= word_count\n",
    "    return text_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "57ff0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Word2Vec(sentences = data_df['Text'], vector_size=5 , window = 3, min_count = 1, sg = 0)\n",
    "\n",
    "#print(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d62a82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in data_df['Text']:\n",
    "    \n",
    "#     print(f\"{text} : {mean_pooling(text, model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e2a53",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c2cedc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Vector for the New Document:\n",
      "[-0.00956134 -0.00233659  0.01236907  0.00481267  0.02602485 -0.02370227\n",
      " -0.02823434  0.0021658   0.02366791  0.00902227  0.01244532 -0.01504472\n",
      " -0.03087573 -0.0311837  -0.00608703  0.02688278 -0.02435123 -0.01122472\n",
      " -0.02898115  0.01942457]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=words, tags=[str(i)]) for i, words in enumerate(tokenized_corpus)]\n",
    "\n",
    "model = Doc2Vec(vector_size=20, window=2, min_count=1, workers=4, epochs=100)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "new_doc = \"This is a new document.\"\n",
    "inferred_vector = model.infer_vector(new_doc.split())\n",
    "\n",
    "print(\"Inferred Vector for the New Document:\")\n",
    "print(inferred_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c805c5",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cf90ecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Vector:\n",
      "[ 0.22598982  0.01075336  0.3115277  -0.15644164 -0.38841882 -0.19919115\n",
      "  0.38567078 -0.19412136  0.18805094  0.23093073 -0.09410948 -0.4862024\n",
      " -0.18364212  0.07392278 -0.13449486  0.02697322 -0.13064033 -0.37826908\n",
      "  0.00733031  0.20476244  0.05846053  0.48386016  0.03605304 -0.2037969\n",
      "  0.02228843  0.41838318  0.60664135  0.30401742  0.07945749  0.8597622\n",
      "  0.18678263  0.0310543   0.13080072 -0.33917683  0.30802834 -0.17839202\n",
      "  0.20650297 -0.45914268  0.01723362  0.56884426 -0.06824379  0.07195051\n",
      " -0.03867647 -0.07933543  0.7110081  -0.36233246  0.32163197 -0.01028265\n",
      "  0.6106154  -0.15750289  0.04591224  0.5149987  -0.6774205  -0.22380434\n",
      "  0.40804994 -0.7133755  -0.3249721   0.35796806  0.07726351 -0.14585829\n",
      "  0.19366181 -0.2993465  -0.6528384  -0.00424204  0.2618007   0.25462076\n",
      "  0.29615134 -0.37594736 -0.01486725 -0.18365957  0.320336   -0.40841898\n",
      " -0.5275203   0.4867237   0.01186347 -0.4025973  -0.03661235 -0.2070368\n",
      " -0.25535536  0.07064598 -0.24805583 -0.01250189  0.00193995 -0.58151174\n",
      "  0.30939204  0.10234646  0.33141315  0.27549073 -0.38183516  0.05960396\n",
      "  0.04645098 -0.50482    -0.03765129  0.24288827 -0.22547828 -0.13389237]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"When is the session going to end again ?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "text_vector = doc.vector\n",
    "\n",
    "print(\"Text Vector:\")\n",
    "print(text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d14384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
